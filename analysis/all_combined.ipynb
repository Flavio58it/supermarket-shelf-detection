{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning \n",
    "### Creating a transfer learning model with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=VGG16(weights='imagenet',include_top=False) #imports the VGG model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(3,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "19 global_average_pooling2d_1\n",
      "20 dense_1\n",
      "21 dense_2\n",
      "22 dense_3\n",
      "23 dense_4\n"
     ]
    }
   ],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "#model.load_weights(\"output_transf_muliclass/new_dataset_model/weights001.h5\")\n",
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x10dedc358>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1113 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory('data2/train/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"output_transf/new_dataset_model/weights-improvement-{epoch:02d}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "mc = keras.callbacks.ModelCheckpoint('output_transf_muliclass/new_dataset_model/weights{epoch:03d}.h5', \n",
    "                                     save_weights_only=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 187 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory('data2/validation/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=1,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 shuffle=False)\n",
    "# 0 > Not stocked\n",
    "# 1 > Other\n",
    "# 2 > Stocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "34/34 [==============================] - 9586s 282s/step - loss: 0.6995 - acc: 0.6853\n",
      "Epoch 2/3\n",
      "34/34 [==============================] - 9859s 290s/step - loss: 0.4029 - acc: 0.8339\n",
      "Epoch 3/3\n",
      "34/34 [==============================] - 11034s 325s/step - loss: 0.3342 - acc: 0.8540\n"
     ]
    }
   ],
   "source": [
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "vgg16_fit = model.fit_generator(generator=train_generator,\n",
    "                               steps_per_epoch=step_size_train,\n",
    "                               epochs=3,\n",
    "                                callbacks = [mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = vgg16_fit.history['acc']\n",
    "#val_loss = vgg16_fit.history['val_loss']\n",
    "plt.plot(loss)\n",
    "#plt.plot(val_loss)\n",
    "#plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"VGG16_transfer_adam_b_crossentropy.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 187 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory('data2/validation/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=1,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = val_generator.filenames\n",
    "nb_samples = len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(val_generator, steps=nb_samples, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6669987248316177, 0.7754010695187166)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"output_transf_muliclass/new_dataset_model/new_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"output_transf_muliclass/new_dataset_model/new_final2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"output_transf_muliclass/new_dataset_model/weights002.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5624859611264724, 0.7754010695187166)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = model.evaluate_generator(val_generator, steps=nb_samples, verbose=0)\n",
    "loss,acc #First 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"output_transf_muliclass/new_dataset_model/new_final2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.engine import InputLayer\n",
    "import cv2\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 105\n",
    "EPSILON = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.engine import InputLayer\n",
    "import keras\n",
    "\n",
    "def to_fully_conv(model):\n",
    "    \"\"\"\n",
    "    This function converts fully connencted layers into convolutional layers for localization.\n",
    "    \n",
    "    \"\"\"\n",
    "    new_model = Sequential()\n",
    "\n",
    "    #input_layer = InputLayer(input_shape=(None, None, 3), name=\"input_new\")\n",
    "\n",
    "    #new_model.add(input_layer)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n",
    "        if \"GlobalAveragePooling2D\" in str(layer):\n",
    "            flattened_ipt = True\n",
    "            f_dim = layer.input_shape\n",
    "\n",
    "        elif \"Dense\" in str(layer):\n",
    "\n",
    "            input_shape = layer.input_shape\n",
    "            output_dim =  layer.get_weights()[1].shape[0]\n",
    "            W,b = layer.get_weights()\n",
    "\n",
    "            if flattened_ipt:\n",
    "                shape = (f_dim[1],f_dim[2],f_dim[3],output_dim)\n",
    "                new_W = W.reshape(shape)\n",
    "                new_layer = Convolution2D(output_dim,\n",
    "                                          (f_dim[1],f_dim[2]),\n",
    "                                          strides=(1,1),\n",
    "                                          activation=layer.activation,\n",
    "                                          padding='valid',\n",
    "                                          weights=[new_W,b])\n",
    "                flattened_ipt = False\n",
    "\n",
    "            else:\n",
    "                shape = (1,1,input_shape[1],output_dim)\n",
    "                new_W = W.reshape(shape)\n",
    "                new_layer = Convolution2D(output_dim,\n",
    "                                          (1,1),\n",
    "                                          strides=(1,1),\n",
    "                                          activation=layer.activation,\n",
    "                                          padding='valid',\n",
    "                                          weights=[new_W,b])\n",
    "        else:\n",
    "            new_layer = layer\n",
    "            \n",
    "        new_model.add(new_layer)\n",
    "        flattened_ipt = False\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /opt/concourse/worker/volumes/live/0851a95b-0a19-4f3a-61a7-502b925fea13/volume/opencv-suite_1535642710601/work/modules/imgproc/src/color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'CvtHelper'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce158f6bd7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_not_stocked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/train/Not_Stocked/not_stocked_0_2371.jpeg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_not_stocked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_not_stocked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg_not_stocked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_not_stocked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_not_stocked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /opt/concourse/worker/volumes/live/0851a95b-0a19-4f3a-61a7-502b925fea13/volume/opencv-suite_1535642710601/work/modules/imgproc/src/color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'CvtHelper'\n"
     ]
    }
   ],
   "source": [
    "example_not_stocked = \"data/train/Not_Stocked/not_stocked_0_2371.jpeg\"\n",
    "img_not_stocked = cv2.imread(example_not_stocked)\n",
    "img_not_stocked = cv2.cvtColor(img_not_stocked, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_not_stocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pred_img(img, w = 105, h = 105):\n",
    "    img = cv2.resize(img,(h,w))\n",
    "    img = img.reshape(1,w,h,3)\n",
    "    return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def localizee(model,filepath = \"\", W = 300, H = 300, less = True):\n",
    "    \"\"\"\n",
    "    This function iterates through scales for a fixed w, h image and gets a heatmap. \n",
    "    The function return a list of bounding over the scales.\n",
    "    It thresholds the heatmap at .7, and appends to a list.\n",
    "    This function follows from: https://github.com/lars76/object-localization/blob/master/example_3/test.py\n",
    "    \"\"\"\n",
    "    scales = np.arange(0.3,1.1, 0.1) #[0.3, 0.4,..., 0.9, 1.0]\n",
    "    list_of_heatmaps = []\n",
    "\n",
    "    THRESHOLD = .5\n",
    "    IMAGE_SIZE = 224\n",
    "    unscaled = cv2.imread(filepath)\n",
    "    \n",
    "    bounding_boxes = [] #return list of bounding boxes w/ corrosponding scale\n",
    "    if unscaled is None:\n",
    "        #No image found\n",
    "        print(\"No such image: \" + filepath)\n",
    "        return\n",
    "    image = cv2.resize(unscaled, (IMAGE_SIZE, IMAGE_SIZE)) #(300,300)\n",
    "    for i,scale in enumerate(scales[::-1]):\n",
    "        #Scale the image\n",
    "        image_copy = image.copy() \n",
    "        unscaled_copy = unscaled.copy()\n",
    "        feat_scaled = process_pred_img(image_copy, w = int(W*scale), h= int(H*scale) )\n",
    "\n",
    "        region = np.squeeze(model.predict(feat_scaled))\n",
    "        output = np.zeros(region[:,:,0].shape, dtype=np.uint8)\n",
    "        if less:\n",
    "            output[region[:,:,0] < THRESHOLD] = 1 \n",
    "            output[region[:,:,0] >= THRESHOLD] = 0\n",
    "        else:\n",
    "            output[region[:,:,0] > THRESHOLD] = 1 \n",
    "            output[region[:,:,0] <= THRESHOLD] = 0\n",
    "            \n",
    "        _,contours, _ = cv2.findContours(output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for cnt in contours:\n",
    "            approx = cv2.approxPolyDP(cnt, EPSILON * cv2.arcLength(cnt, True), True)\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            try:\n",
    "                #Sometimes output is a scalar and has no shape\n",
    "                x0 = np.rint(x * unscaled.shape[1] / output.shape[1]).astype(int)\n",
    "                x1 = np.rint((x + w) * unscaled.shape[1] / output.shape[1]).astype(int)\n",
    "                y0 = np.rint(y * unscaled.shape[0] / output.shape[0]).astype(int)\n",
    "                y1 = np.rint((y + h) * unscaled.shape[0] / output.shape[0]).astype(int)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            \n",
    "            bounding_boxes.append((x0, y0, x1, y1))\n",
    "            cv2.rectangle(unscaled_copy, (x0, y0), (x1, y1), (255, 0, 255), 10)\n",
    "        cv2.imwrite(\"localized_sample/localized_INDEX_\" + str(i) + \"_FILE_\" + filepath.split(\"/\")[-1], unscaled_copy)\n",
    "\n",
    "    return np.array(bounding_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "localized = localizee(model=new_model, \n",
    "         filepath = filename2,\n",
    "         W = 1000, \n",
    "         H = 1000)\n",
    "\n",
    "\n",
    "\n",
    "localized2 = localizee(model=new_model, \n",
    "         filepath = filename2,\n",
    "         W = 1000, \n",
    "         H = 1000, \n",
    "        less = False)\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malisiewicz et al.\n",
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    " \n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    " \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    " \n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    " \n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    " \n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
